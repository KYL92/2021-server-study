{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T13:54:24.976780Z",
     "start_time": "2021-08-02T13:54:24.149631Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# GPU 자원 사용확인\n",
    "devices_id = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(\n",
    "    devices_id\n",
    ")  # fix bug for `ERROR: all tensors must be on devices[0]`\n",
    "\n",
    "# Create Tensorboard SummaryWriter instance\n",
    "writer = SummaryWriter('./summary/with_activation')\n",
    "\n",
    "# Step 1. Load Dataset\n",
    "train_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=True, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "test_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=False, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "\n",
    "# Step 2. Make Dataset Iterable\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T13:54:25.508601Z",
     "start_time": "2021-08-02T13:54:25.502754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3. Create Model Class\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 300)\n",
    "        self.linear2 = torch.nn.Linear(300, int(input_dim / 4))  # 392x196\n",
    "        self.linear3 = torch.nn.Linear(int(input_dim / 4), output_dim)  # 196x10\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = F.relu(self.linear1(x))\n",
    "        outputs = F.relu(self.linear2(outputs))\n",
    "        outputs = self.linear3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T13:54:29.151744Z",
     "start_time": "2021-08-02T13:54:25.905504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "# [test] 만일 MSE을 LOSS 함수로 쓴다면???\n",
    "# output_dim = 1\n",
    "lr_rate = 0.01\n",
    "\n",
    "# Step 4. Instantiate Model Class\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model = nn.DataParallel(model, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model = nn.DataParallel(model, device_ids=[devices_id]).cuda()\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss()  # computes softmax and then the cross entropy\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T13:57:25.836303Z",
     "start_time": "2021-08-02T13:54:29.152907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] [Iteration: 199/600] [Loss: 2.223] [Accuracy: 50.44]\n",
      "[Epoch 0] [Iteration: 399/600] [Loss: 1.974] [Accuracy: 68.64]\n",
      "[Epoch 0] [Iteration: 599/600] [Loss: 1.475] [Accuracy: 73.39]\n",
      "[Epoch 1] [Iteration: 199/600] [Loss: 1.002] [Accuracy: 80.20]\n",
      "[Epoch 1] [Iteration: 399/600] [Loss: 0.637] [Accuracy: 84.17]\n",
      "[Epoch 1] [Iteration: 599/600] [Loss: 0.519] [Accuracy: 86.24]\n",
      "[Epoch 2] [Iteration: 199/600] [Loss: 0.570] [Accuracy: 87.64]\n",
      "[Epoch 2] [Iteration: 399/600] [Loss: 0.415] [Accuracy: 88.32]\n",
      "[Epoch 2] [Iteration: 599/600] [Loss: 0.315] [Accuracy: 88.87]\n",
      "[Epoch 3] [Iteration: 199/600] [Loss: 0.370] [Accuracy: 89.21]\n",
      "[Epoch 3] [Iteration: 399/600] [Loss: 0.508] [Accuracy: 89.66]\n",
      "[Epoch 3] [Iteration: 599/600] [Loss: 0.263] [Accuracy: 89.75]\n",
      "[Epoch 4] [Iteration: 199/600] [Loss: 0.394] [Accuracy: 90.26]\n",
      "[Epoch 4] [Iteration: 399/600] [Loss: 0.352] [Accuracy: 90.50]\n",
      "[Epoch 4] [Iteration: 599/600] [Loss: 0.447] [Accuracy: 90.55]\n",
      "[Epoch 5] [Iteration: 199/600] [Loss: 0.315] [Accuracy: 90.73]\n",
      "[Epoch 5] [Iteration: 399/600] [Loss: 0.244] [Accuracy: 90.93]\n",
      "[Epoch 5] [Iteration: 599/600] [Loss: 0.412] [Accuracy: 91.11]\n",
      "[Epoch 6] [Iteration: 199/600] [Loss: 0.231] [Accuracy: 91.29]\n",
      "[Epoch 6] [Iteration: 399/600] [Loss: 0.255] [Accuracy: 91.43]\n",
      "[Epoch 6] [Iteration: 599/600] [Loss: 0.257] [Accuracy: 91.71]\n",
      "[Epoch 7] [Iteration: 199/600] [Loss: 0.318] [Accuracy: 91.54]\n",
      "[Epoch 7] [Iteration: 399/600] [Loss: 0.248] [Accuracy: 91.81]\n",
      "[Epoch 7] [Iteration: 599/600] [Loss: 0.248] [Accuracy: 92.01]\n",
      "[Epoch 8] [Iteration: 199/600] [Loss: 0.283] [Accuracy: 92.03]\n",
      "[Epoch 8] [Iteration: 399/600] [Loss: 0.309] [Accuracy: 92.18]\n",
      "[Epoch 8] [Iteration: 599/600] [Loss: 0.491] [Accuracy: 92.31]\n",
      "[Epoch 9] [Iteration: 199/600] [Loss: 0.224] [Accuracy: 92.47]\n",
      "[Epoch 9] [Iteration: 399/600] [Loss: 0.107] [Accuracy: 92.55]\n",
      "[Epoch 9] [Iteration: 599/600] [Loss: 0.261] [Accuracy: 92.49]\n",
      "[Epoch 10] [Iteration: 199/600] [Loss: 0.188] [Accuracy: 92.69]\n",
      "[Epoch 10] [Iteration: 399/600] [Loss: 0.470] [Accuracy: 92.84]\n",
      "[Epoch 10] [Iteration: 599/600] [Loss: 0.207] [Accuracy: 92.92]\n",
      "[Epoch 11] [Iteration: 199/600] [Loss: 0.315] [Accuracy: 93.02]\n",
      "[Epoch 11] [Iteration: 399/600] [Loss: 0.260] [Accuracy: 93.13]\n",
      "[Epoch 11] [Iteration: 599/600] [Loss: 0.150] [Accuracy: 93.09]\n",
      "[Epoch 12] [Iteration: 199/600] [Loss: 0.212] [Accuracy: 93.10]\n",
      "[Epoch 12] [Iteration: 399/600] [Loss: 0.167] [Accuracy: 93.32]\n",
      "[Epoch 12] [Iteration: 599/600] [Loss: 0.168] [Accuracy: 93.44]\n",
      "[Epoch 13] [Iteration: 199/600] [Loss: 0.143] [Accuracy: 93.48]\n",
      "[Epoch 13] [Iteration: 399/600] [Loss: 0.170] [Accuracy: 93.49]\n",
      "[Epoch 13] [Iteration: 599/600] [Loss: 0.309] [Accuracy: 93.66]\n",
      "[Epoch 14] [Iteration: 199/600] [Loss: 0.198] [Accuracy: 93.56]\n",
      "[Epoch 14] [Iteration: 399/600] [Loss: 0.076] [Accuracy: 93.83]\n",
      "[Epoch 14] [Iteration: 599/600] [Loss: 0.241] [Accuracy: 93.80]\n",
      "[Epoch 15] [Iteration: 199/600] [Loss: 0.219] [Accuracy: 93.85]\n",
      "[Epoch 15] [Iteration: 399/600] [Loss: 0.210] [Accuracy: 93.92]\n",
      "[Epoch 15] [Iteration: 599/600] [Loss: 0.197] [Accuracy: 94.07]\n",
      "[Epoch 16] [Iteration: 199/600] [Loss: 0.298] [Accuracy: 93.96]\n",
      "[Epoch 16] [Iteration: 399/600] [Loss: 0.220] [Accuracy: 94.15]\n",
      "[Epoch 16] [Iteration: 599/600] [Loss: 0.164] [Accuracy: 94.20]\n",
      "[Epoch 17] [Iteration: 199/600] [Loss: 0.211] [Accuracy: 94.24]\n",
      "[Epoch 17] [Iteration: 399/600] [Loss: 0.213] [Accuracy: 94.29]\n",
      "[Epoch 17] [Iteration: 599/600] [Loss: 0.161] [Accuracy: 94.26]\n",
      "[Epoch 18] [Iteration: 199/600] [Loss: 0.308] [Accuracy: 94.34]\n",
      "[Epoch 18] [Iteration: 399/600] [Loss: 0.113] [Accuracy: 94.37]\n",
      "[Epoch 18] [Iteration: 599/600] [Loss: 0.160] [Accuracy: 94.57]\n",
      "[Epoch 19] [Iteration: 199/600] [Loss: 0.088] [Accuracy: 94.49]\n",
      "[Epoch 19] [Iteration: 399/600] [Loss: 0.212] [Accuracy: 94.61]\n",
      "[Epoch 19] [Iteration: 599/600] [Loss: 0.133] [Accuracy: 94.53]\n",
      "[Epoch 20] [Iteration: 199/600] [Loss: 0.216] [Accuracy: 94.62]\n",
      "[Epoch 20] [Iteration: 399/600] [Loss: 0.186] [Accuracy: 94.81]\n",
      "[Epoch 20] [Iteration: 599/600] [Loss: 0.168] [Accuracy: 94.80]\n",
      "[Epoch 21] [Iteration: 199/600] [Loss: 0.262] [Accuracy: 94.90]\n",
      "[Epoch 21] [Iteration: 399/600] [Loss: 0.321] [Accuracy: 94.91]\n",
      "[Epoch 21] [Iteration: 599/600] [Loss: 0.069] [Accuracy: 94.92]\n",
      "[Epoch 22] [Iteration: 199/600] [Loss: 0.181] [Accuracy: 95.14]\n",
      "[Epoch 22] [Iteration: 399/600] [Loss: 0.133] [Accuracy: 95.04]\n",
      "[Epoch 22] [Iteration: 599/600] [Loss: 0.158] [Accuracy: 95.16]\n",
      "[Epoch 23] [Iteration: 199/600] [Loss: 0.235] [Accuracy: 95.16]\n",
      "[Epoch 23] [Iteration: 399/600] [Loss: 0.095] [Accuracy: 95.21]\n",
      "[Epoch 23] [Iteration: 599/600] [Loss: 0.124] [Accuracy: 95.26]\n",
      "[Epoch 24] [Iteration: 199/600] [Loss: 0.181] [Accuracy: 95.16]\n",
      "[Epoch 24] [Iteration: 399/600] [Loss: 0.177] [Accuracy: 95.29]\n",
      "[Epoch 24] [Iteration: 599/600] [Loss: 0.086] [Accuracy: 95.39]\n",
      "[Epoch 25] [Iteration: 199/600] [Loss: 0.167] [Accuracy: 95.46]\n",
      "[Epoch 25] [Iteration: 399/600] [Loss: 0.120] [Accuracy: 95.55]\n",
      "[Epoch 25] [Iteration: 599/600] [Loss: 0.144] [Accuracy: 95.41]\n",
      "[Epoch 26] [Iteration: 199/600] [Loss: 0.103] [Accuracy: 95.55]\n",
      "[Epoch 26] [Iteration: 399/600] [Loss: 0.101] [Accuracy: 95.62]\n",
      "[Epoch 26] [Iteration: 599/600] [Loss: 0.298] [Accuracy: 95.63]\n",
      "[Epoch 27] [Iteration: 199/600] [Loss: 0.236] [Accuracy: 95.69]\n",
      "[Epoch 27] [Iteration: 399/600] [Loss: 0.083] [Accuracy: 95.75]\n",
      "[Epoch 27] [Iteration: 599/600] [Loss: 0.111] [Accuracy: 95.78]\n",
      "[Epoch 28] [Iteration: 199/600] [Loss: 0.125] [Accuracy: 95.68]\n",
      "[Epoch 28] [Iteration: 399/600] [Loss: 0.129] [Accuracy: 95.87]\n",
      "[Epoch 28] [Iteration: 599/600] [Loss: 0.086] [Accuracy: 95.91]\n",
      "[Epoch 29] [Iteration: 199/600] [Loss: 0.133] [Accuracy: 95.97]\n",
      "[Epoch 29] [Iteration: 399/600] [Loss: 0.155] [Accuracy: 95.93]\n",
      "[Epoch 29] [Iteration: 599/600] [Loss: 0.247] [Accuracy: 95.94]\n"
     ]
    }
   ],
   "source": [
    "# Step 7. Train Model\n",
    "\n",
    "# 임의의 학습 이미지를 가져옵니다\n",
    "dataiter = iter(train_loader)\n",
    "images, _ = dataiter.next()\n",
    "writer.add_graph(model, images.view(-1, 28 * 28))\n",
    "\n",
    "loss = 0\n",
    "total_iter = 0\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        labels = labels\n",
    "\n",
    "        images = images.to(devices_id)\n",
    "        labels = labels.to(devices_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_iter += 1\n",
    "        writer.add_scalar('Train/Loss', loss, total_iter)\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28 * 28)\n",
    "                images = images.to(devices_id)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                predicted = predicted.cpu()\n",
    "                correct += (predicted == labels).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\n",
    "                f\"[Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss.item():.3f}] [Accuracy: {accuracy:.2f}]\"\n",
    "            )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T14:05:42.753450Z",
     "start_time": "2021-08-02T14:05:42.719176Z"
    }
   },
   "outputs": [],
   "source": [
    "# create checkpoint variable and add important data\n",
    "checkpoint = {\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"./mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T14:10:08.279131Z",
     "start_time": "2021-08-02T14:10:08.171224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'linear3.weight', 'linear3.bias'], unexpected_keys=['module.linear1.weight', 'module.linear1.bias', 'module.linear2.weight', 'module.linear2.bias', 'module.linear3.weight', 'module.linear3.bias'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. Make Dataset Iterable\n",
    "def get_state_dict(origin_dict):\n",
    "    old_keys = origin_dict.keys()\n",
    "    new_dict = {}\n",
    "    \n",
    "    for ii in old_keys:\n",
    "        temp_key = str(ii)\n",
    "        if temp_key[0:7] == \"module.\":\n",
    "            new_key = temp_key[7:]\n",
    "        else:\n",
    "            new_key = temp_key\n",
    "            \n",
    "        new_dict[new_key] = origin_dict[temp_key]\n",
    "    return new_dict\n",
    "\n",
    "# model load \n",
    "onnx_model = LogisticRegression(input_dim, output_dim)\n",
    "onnx_model.to(devices_id)\n",
    "onnx_model.eval()\n",
    "\n",
    "checkpoint = torch.load(\"mnist.pt\", map_location=devices_id)\n",
    "checkpoint_dict = get_state_dict(checkpoint[\"state_dict\"])\n",
    "onnx_model.load_state_dict(checkpoint_dict,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T14:11:12.982904Z",
     "start_time": "2021-08-02T14:11:12.916074Z"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model.to('cpu')\n",
    "onnx_model.eval()\n",
    "# 모델에 대한 입력값\n",
    "x = torch.randn(1, 28*28, requires_grad=True)\n",
    "torch_out = onnx_model(x)\n",
    "\n",
    "# 모델 변환\n",
    "torch.onnx.export(onnx_model,               # 실행될 모델\n",
    "                  x,                         # 모델 입력값 (튜플 또는 여러 입력값들도 가능)\n",
    "                  \"mnist.onnx\",   # 모델 저장 경로 (파일 또는 파일과 유사한 객체 모두 가능)\n",
    "                  export_params=True,        # 모델 파일 안에 학습된 모델 가중치를 저장할지의 여부\n",
    "                  opset_version=10,          # 모델을 변환할 때 사용할 ONNX 버전\n",
    "                  do_constant_folding=True,  # 최적화시 상수폴딩을 사용할지의 여부\n",
    "                  input_names = ['input'],   # 모델의 입력값을 가리키는 이름\n",
    "                  output_names = ['output'], # 모델의 출력값을 가리키는 이름\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # 가변적인 길이를 가진 차원\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
