{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# GPU 자원 사용확인\n",
    "devices_id = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(\n",
    "    devices_id\n",
    ")  # fix bug for `ERROR: all tensors must be on devices[0]`\n",
    "\n",
    "# Create Tensorboard SummaryWriter instance\n",
    "writer = SummaryWriter('./summary/with_activation')\n",
    "\n",
    "# Step 1. Load Dataset\n",
    "train_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=True, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "test_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=False, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "\n",
    "# Step 2. Make Dataset Iterable\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create Model Class\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 300)\n",
    "        self.linear2 = torch.nn.Linear(300, int(input_dim / 4))  # 392x196\n",
    "        self.linear3 = torch.nn.Linear(int(input_dim / 4), output_dim)  # 196x10\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = F.relu(self.linear1(x))\n",
    "        outputs = F.relu(self.linear2(outputs))\n",
    "        outputs = self.linear3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "# [test] 만일 MSE을 LOSS 함수로 쓴다면???\n",
    "# output_dim = 1\n",
    "lr_rate = 0.01\n",
    "\n",
    "# Step 4. Instantiate Model Class\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model = nn.DataParallel(model, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model = nn.DataParallel(model, device_ids=[devices_id]).cuda()\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss()  # computes softmax and then the cross entropy\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0] [Iteration: 199/600] [Loss: 2.216] [Accuracy: 44.95]\n",
      "[Epoch 0] [Iteration: 399/600] [Loss: 1.964] [Accuracy: 56.87]\n",
      "[Epoch 0] [Iteration: 599/600] [Loss: 1.397] [Accuracy: 70.14]\n",
      "[Epoch 1] [Iteration: 199/600] [Loss: 0.910] [Accuracy: 80.78]\n",
      "[Epoch 1] [Iteration: 399/600] [Loss: 0.733] [Accuracy: 83.93]\n",
      "[Epoch 1] [Iteration: 599/600] [Loss: 0.537] [Accuracy: 85.83]\n",
      "[Epoch 2] [Iteration: 199/600] [Loss: 0.404] [Accuracy: 87.18]\n",
      "[Epoch 2] [Iteration: 399/600] [Loss: 0.379] [Accuracy: 88.29]\n",
      "[Epoch 2] [Iteration: 599/600] [Loss: 0.496] [Accuracy: 89.01]\n",
      "[Epoch 3] [Iteration: 199/600] [Loss: 0.421] [Accuracy: 89.32]\n",
      "[Epoch 3] [Iteration: 399/600] [Loss: 0.443] [Accuracy: 89.58]\n",
      "[Epoch 3] [Iteration: 599/600] [Loss: 0.437] [Accuracy: 89.95]\n",
      "[Epoch 4] [Iteration: 199/600] [Loss: 0.396] [Accuracy: 90.42]\n",
      "[Epoch 4] [Iteration: 399/600] [Loss: 0.440] [Accuracy: 90.47]\n",
      "[Epoch 4] [Iteration: 599/600] [Loss: 0.317] [Accuracy: 90.66]\n",
      "[Epoch 5] [Iteration: 199/600] [Loss: 0.319] [Accuracy: 90.74]\n",
      "[Epoch 5] [Iteration: 399/600] [Loss: 0.194] [Accuracy: 91.20]\n",
      "[Epoch 5] [Iteration: 599/600] [Loss: 0.227] [Accuracy: 91.31]\n",
      "[Epoch 6] [Iteration: 199/600] [Loss: 0.416] [Accuracy: 91.24]\n",
      "[Epoch 6] [Iteration: 399/600] [Loss: 0.517] [Accuracy: 91.48]\n",
      "[Epoch 6] [Iteration: 599/600] [Loss: 0.137] [Accuracy: 91.72]\n",
      "[Epoch 7] [Iteration: 199/600] [Loss: 0.273] [Accuracy: 91.81]\n",
      "[Epoch 7] [Iteration: 399/600] [Loss: 0.390] [Accuracy: 91.93]\n",
      "[Epoch 7] [Iteration: 599/600] [Loss: 0.171] [Accuracy: 92.07]\n",
      "[Epoch 8] [Iteration: 199/600] [Loss: 0.256] [Accuracy: 92.21]\n",
      "[Epoch 8] [Iteration: 399/600] [Loss: 0.204] [Accuracy: 92.36]\n",
      "[Epoch 8] [Iteration: 599/600] [Loss: 0.189] [Accuracy: 92.37]\n",
      "[Epoch 9] [Iteration: 199/600] [Loss: 0.215] [Accuracy: 92.70]\n",
      "[Epoch 9] [Iteration: 399/600] [Loss: 0.303] [Accuracy: 92.64]\n",
      "[Epoch 9] [Iteration: 599/600] [Loss: 0.372] [Accuracy: 92.80]\n",
      "[Epoch 10] [Iteration: 199/600] [Loss: 0.248] [Accuracy: 92.88]\n",
      "[Epoch 10] [Iteration: 399/600] [Loss: 0.231] [Accuracy: 93.16]\n",
      "[Epoch 10] [Iteration: 599/600] [Loss: 0.266] [Accuracy: 93.14]\n",
      "[Epoch 11] [Iteration: 199/600] [Loss: 0.378] [Accuracy: 93.21]\n",
      "[Epoch 11] [Iteration: 399/600] [Loss: 0.268] [Accuracy: 93.27]\n",
      "[Epoch 11] [Iteration: 599/600] [Loss: 0.144] [Accuracy: 93.52]\n",
      "[Epoch 12] [Iteration: 199/600] [Loss: 0.197] [Accuracy: 93.57]\n",
      "[Epoch 12] [Iteration: 399/600] [Loss: 0.193] [Accuracy: 93.61]\n",
      "[Epoch 12] [Iteration: 599/600] [Loss: 0.214] [Accuracy: 93.77]\n",
      "[Epoch 13] [Iteration: 199/600] [Loss: 0.279] [Accuracy: 93.70]\n",
      "[Epoch 13] [Iteration: 399/600] [Loss: 0.298] [Accuracy: 93.75]\n",
      "[Epoch 13] [Iteration: 599/600] [Loss: 0.106] [Accuracy: 93.79]\n",
      "[Epoch 14] [Iteration: 199/600] [Loss: 0.255] [Accuracy: 93.71]\n",
      "[Epoch 14] [Iteration: 399/600] [Loss: 0.174] [Accuracy: 93.75]\n",
      "[Epoch 14] [Iteration: 599/600] [Loss: 0.196] [Accuracy: 93.90]\n",
      "[Epoch 15] [Iteration: 199/600] [Loss: 0.216] [Accuracy: 93.91]\n",
      "[Epoch 15] [Iteration: 399/600] [Loss: 0.126] [Accuracy: 94.10]\n",
      "[Epoch 15] [Iteration: 599/600] [Loss: 0.273] [Accuracy: 94.02]\n",
      "[Epoch 16] [Iteration: 199/600] [Loss: 0.196] [Accuracy: 94.17]\n",
      "[Epoch 16] [Iteration: 399/600] [Loss: 0.156] [Accuracy: 94.21]\n",
      "[Epoch 16] [Iteration: 599/600] [Loss: 0.156] [Accuracy: 94.27]\n",
      "[Epoch 17] [Iteration: 199/600] [Loss: 0.246] [Accuracy: 94.25]\n",
      "[Epoch 17] [Iteration: 399/600] [Loss: 0.276] [Accuracy: 94.41]\n",
      "[Epoch 17] [Iteration: 599/600] [Loss: 0.191] [Accuracy: 94.35]\n",
      "[Epoch 18] [Iteration: 199/600] [Loss: 0.166] [Accuracy: 94.47]\n",
      "[Epoch 18] [Iteration: 399/600] [Loss: 0.166] [Accuracy: 94.60]\n",
      "[Epoch 18] [Iteration: 599/600] [Loss: 0.112] [Accuracy: 94.62]\n",
      "[Epoch 19] [Iteration: 199/600] [Loss: 0.165] [Accuracy: 94.64]\n",
      "[Epoch 19] [Iteration: 399/600] [Loss: 0.103] [Accuracy: 94.81]\n",
      "[Epoch 19] [Iteration: 599/600] [Loss: 0.153] [Accuracy: 94.78]\n",
      "[Epoch 20] [Iteration: 199/600] [Loss: 0.061] [Accuracy: 94.78]\n",
      "[Epoch 20] [Iteration: 399/600] [Loss: 0.157] [Accuracy: 94.80]\n",
      "[Epoch 20] [Iteration: 599/600] [Loss: 0.174] [Accuracy: 95.01]\n",
      "[Epoch 21] [Iteration: 199/600] [Loss: 0.159] [Accuracy: 94.89]\n",
      "[Epoch 21] [Iteration: 399/600] [Loss: 0.164] [Accuracy: 95.05]\n",
      "[Epoch 21] [Iteration: 599/600] [Loss: 0.261] [Accuracy: 95.01]\n",
      "[Epoch 22] [Iteration: 199/600] [Loss: 0.109] [Accuracy: 95.08]\n",
      "[Epoch 22] [Iteration: 399/600] [Loss: 0.168] [Accuracy: 95.11]\n",
      "[Epoch 22] [Iteration: 599/600] [Loss: 0.162] [Accuracy: 95.21]\n",
      "[Epoch 23] [Iteration: 199/600] [Loss: 0.075] [Accuracy: 95.20]\n",
      "[Epoch 23] [Iteration: 399/600] [Loss: 0.208] [Accuracy: 95.32]\n",
      "[Epoch 23] [Iteration: 599/600] [Loss: 0.117] [Accuracy: 95.34]\n",
      "[Epoch 24] [Iteration: 199/600] [Loss: 0.102] [Accuracy: 95.39]\n",
      "[Epoch 24] [Iteration: 399/600] [Loss: 0.174] [Accuracy: 95.45]\n",
      "[Epoch 24] [Iteration: 599/600] [Loss: 0.102] [Accuracy: 95.41]\n",
      "[Epoch 25] [Iteration: 199/600] [Loss: 0.156] [Accuracy: 95.56]\n",
      "[Epoch 25] [Iteration: 399/600] [Loss: 0.280] [Accuracy: 95.40]\n",
      "[Epoch 25] [Iteration: 599/600] [Loss: 0.160] [Accuracy: 95.60]\n",
      "[Epoch 26] [Iteration: 199/600] [Loss: 0.240] [Accuracy: 95.72]\n",
      "[Epoch 26] [Iteration: 399/600] [Loss: 0.109] [Accuracy: 95.57]\n",
      "[Epoch 26] [Iteration: 599/600] [Loss: 0.165] [Accuracy: 95.60]\n",
      "[Epoch 27] [Iteration: 199/600] [Loss: 0.183] [Accuracy: 95.63]\n",
      "[Epoch 27] [Iteration: 399/600] [Loss: 0.264] [Accuracy: 95.78]\n",
      "[Epoch 27] [Iteration: 599/600] [Loss: 0.157] [Accuracy: 95.81]\n",
      "[Epoch 28] [Iteration: 199/600] [Loss: 0.149] [Accuracy: 95.77]\n",
      "[Epoch 28] [Iteration: 399/600] [Loss: 0.105] [Accuracy: 95.89]\n",
      "[Epoch 28] [Iteration: 599/600] [Loss: 0.100] [Accuracy: 95.92]\n",
      "[Epoch 29] [Iteration: 199/600] [Loss: 0.078] [Accuracy: 95.94]\n",
      "[Epoch 29] [Iteration: 399/600] [Loss: 0.052] [Accuracy: 95.88]\n",
      "[Epoch 29] [Iteration: 599/600] [Loss: 0.049] [Accuracy: 95.93]\n"
     ]
    }
   ],
   "source": [
    "# Step 7. Train Model\n",
    "\n",
    "# 임의의 학습 이미지를 가져옵니다\n",
    "dataiter = iter(train_loader)\n",
    "images, _ = dataiter.next()\n",
    "writer.add_graph(model, images.view(-1, 28 * 28))\n",
    "\n",
    "loss = 0\n",
    "total_iter = 0\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        labels = labels\n",
    "\n",
    "        images = images.to(devices_id)\n",
    "        labels = labels.to(devices_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_iter += 1\n",
    "        writer.add_scalar('Train/Loss', loss, total_iter)\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28 * 28)\n",
    "                images = images.to(devices_id)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                predicted = predicted.cpu()\n",
    "                correct += (predicted == labels).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\n",
    "                f\"[Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss.item():.3f}] [Accuracy: {accuracy:.2f}]\"\n",
    "            )\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}