{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# GPU 자원 사용확인\n",
    "devices_id = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(\n",
    "    devices_id\n",
    ")  # fix bug for `ERROR: all tensors must be on devices[0]`\n",
    "\n",
    "# Create Tensorboard SummaryWriter instance\n",
    "writer_deep = SummaryWriter('./summary/deep_without_activation')\n",
    "writer_shallow = SummaryWriter('./summary/shallow_without_activation')\n",
    "\n",
    "# Step 1. Load Dataset\n",
    "train_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=True, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "test_dataset = dsets.MNIST(\n",
    "    root=\"../data\", train=False, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "\n",
    "# Step 2. Make Dataset Iterable\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Create Model Class\n",
    "class Deep_LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Deep_LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 300)\n",
    "        self.linear2 = torch.nn.Linear(300, int(input_dim / 4))  # 392x196\n",
    "        self.linear3 = torch.nn.Linear(int(input_dim / 4), output_dim)  # 196x10\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear1(x)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.linear3(outputs)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Step 3. Create Model Class\n",
    "class Shallow_LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Shallow_LogisticRegression, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear1(x)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "# [test] 만일 MSE을 LOSS 함수로 쓴다면???\n",
    "# output_dim = 1\n",
    "lr_rate = 0.01\n",
    "\n",
    "# Step 4. Instantiate Model Class\n",
    "model_deep = Deep_LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model_deep = nn.DataParallel(model_deep, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model_deep = nn.DataParallel(model_deep, device_ids=[devices_id]).cuda()\n",
    "\n",
    "model_shallow = Shallow_LogisticRegression(input_dim, output_dim)\n",
    "if devices_id == type([]):  # -> GPU\n",
    "    model_shallow = nn.DataParallel(model_shallow, device_ids=devices_id).cuda()\n",
    "else:\n",
    "    model_shallow = nn.DataParallel(model_shallow, device_ids=[devices_id]).cuda()\n",
    "\n",
    "# Step 5. Instantiate Loss Class\n",
    "criterion = torch.nn.CrossEntropyLoss()  # computes softmax and then the cross entropy\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "optimizer_deep = torch.optim.SGD(model_deep.parameters(), lr=lr_rate)\n",
    "optimizer_shallow = torch.optim.SGD(model_shallow.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Deep] [Epoch 0] [Iteration: 199/600] [Loss: 1.869] [Accuracy: 72.24]\n",
      "[Shal] [Epoch 0] [Iteration: 199/600] [Loss: 1.284] [Accuracy: 80.36]\n",
      "[Deep] [Epoch 0] [Iteration: 399/600] [Loss: 1.148] [Accuracy: 77.10]\n",
      "[Shal] [Epoch 0] [Iteration: 399/600] [Loss: 0.933] [Accuracy: 84.13]\n",
      "[Deep] [Epoch 0] [Iteration: 599/600] [Loss: 0.840] [Accuracy: 82.89]\n",
      "[Shal] [Epoch 0] [Iteration: 599/600] [Loss: 0.841] [Accuracy: 85.89]\n",
      "[Deep] [Epoch 1] [Iteration: 199/600] [Loss: 0.589] [Accuracy: 84.89]\n",
      "[Shal] [Epoch 1] [Iteration: 199/600] [Loss: 0.629] [Accuracy: 86.49]\n",
      "[Deep] [Epoch 1] [Iteration: 399/600] [Loss: 0.699] [Accuracy: 86.38]\n",
      "[Shal] [Epoch 1] [Iteration: 399/600] [Loss: 0.757] [Accuracy: 87.18]\n",
      "[Deep] [Epoch 1] [Iteration: 599/600] [Loss: 0.446] [Accuracy: 87.47]\n",
      "[Shal] [Epoch 1] [Iteration: 599/600] [Loss: 0.504] [Accuracy: 87.51]\n",
      "[Deep] [Epoch 2] [Iteration: 199/600] [Loss: 0.463] [Accuracy: 88.41]\n",
      "[Shal] [Epoch 2] [Iteration: 199/600] [Loss: 0.549] [Accuracy: 87.76]\n",
      "[Deep] [Epoch 2] [Iteration: 399/600] [Loss: 0.533] [Accuracy: 88.85]\n",
      "[Shal] [Epoch 2] [Iteration: 399/600] [Loss: 0.629] [Accuracy: 88.01]\n",
      "[Deep] [Epoch 2] [Iteration: 599/600] [Loss: 0.234] [Accuracy: 89.31]\n",
      "[Shal] [Epoch 2] [Iteration: 599/600] [Loss: 0.376] [Accuracy: 88.21]\n",
      "[Deep] [Epoch 3] [Iteration: 199/600] [Loss: 0.288] [Accuracy: 89.44]\n",
      "[Shal] [Epoch 3] [Iteration: 199/600] [Loss: 0.442] [Accuracy: 88.38]\n",
      "[Deep] [Epoch 3] [Iteration: 399/600] [Loss: 0.422] [Accuracy: 89.81]\n",
      "[Shal] [Epoch 3] [Iteration: 399/600] [Loss: 0.515] [Accuracy: 88.61]\n",
      "[Deep] [Epoch 3] [Iteration: 599/600] [Loss: 0.280] [Accuracy: 90.01]\n",
      "[Shal] [Epoch 3] [Iteration: 599/600] [Loss: 0.369] [Accuracy: 88.77]\n",
      "[Deep] [Epoch 4] [Iteration: 199/600] [Loss: 0.289] [Accuracy: 89.89]\n",
      "[Shal] [Epoch 4] [Iteration: 199/600] [Loss: 0.397] [Accuracy: 88.96]\n",
      "[Deep] [Epoch 4] [Iteration: 399/600] [Loss: 0.209] [Accuracy: 90.20]\n",
      "[Shal] [Epoch 4] [Iteration: 399/600] [Loss: 0.332] [Accuracy: 89.14]\n",
      "[Deep] [Epoch 4] [Iteration: 599/600] [Loss: 0.345] [Accuracy: 90.42]\n",
      "[Shal] [Epoch 4] [Iteration: 599/600] [Loss: 0.414] [Accuracy: 89.20]\n",
      "[Deep] [Epoch 5] [Iteration: 199/600] [Loss: 0.411] [Accuracy: 90.65]\n",
      "[Shal] [Epoch 5] [Iteration: 199/600] [Loss: 0.494] [Accuracy: 89.29]\n",
      "[Deep] [Epoch 5] [Iteration: 399/600] [Loss: 0.358] [Accuracy: 90.76]\n",
      "[Shal] [Epoch 5] [Iteration: 399/600] [Loss: 0.459] [Accuracy: 89.37]\n",
      "[Deep] [Epoch 5] [Iteration: 599/600] [Loss: 0.349] [Accuracy: 90.83]\n",
      "[Shal] [Epoch 5] [Iteration: 599/600] [Loss: 0.440] [Accuracy: 89.48]\n",
      "[Deep] [Epoch 6] [Iteration: 199/600] [Loss: 0.260] [Accuracy: 90.88]\n",
      "[Shal] [Epoch 6] [Iteration: 199/600] [Loss: 0.421] [Accuracy: 89.55]\n",
      "[Deep] [Epoch 6] [Iteration: 399/600] [Loss: 0.343] [Accuracy: 91.17]\n",
      "[Shal] [Epoch 6] [Iteration: 399/600] [Loss: 0.463] [Accuracy: 89.67]\n",
      "[Deep] [Epoch 6] [Iteration: 599/600] [Loss: 0.437] [Accuracy: 91.28]\n",
      "[Shal] [Epoch 6] [Iteration: 599/600] [Loss: 0.488] [Accuracy: 89.84]\n",
      "[Deep] [Epoch 7] [Iteration: 199/600] [Loss: 0.287] [Accuracy: 91.20]\n",
      "[Shal] [Epoch 7] [Iteration: 199/600] [Loss: 0.355] [Accuracy: 89.77]\n",
      "[Deep] [Epoch 7] [Iteration: 399/600] [Loss: 0.496] [Accuracy: 91.36]\n",
      "[Shal] [Epoch 7] [Iteration: 399/600] [Loss: 0.505] [Accuracy: 89.89]\n",
      "[Deep] [Epoch 7] [Iteration: 599/600] [Loss: 0.430] [Accuracy: 91.16]\n",
      "[Shal] [Epoch 7] [Iteration: 599/600] [Loss: 0.511] [Accuracy: 89.92]\n",
      "[Deep] [Epoch 8] [Iteration: 199/600] [Loss: 0.263] [Accuracy: 91.49]\n",
      "[Shal] [Epoch 8] [Iteration: 199/600] [Loss: 0.329] [Accuracy: 90.02]\n",
      "[Deep] [Epoch 8] [Iteration: 399/600] [Loss: 0.509] [Accuracy: 91.45]\n",
      "[Shal] [Epoch 8] [Iteration: 399/600] [Loss: 0.554] [Accuracy: 90.06]\n",
      "[Deep] [Epoch 8] [Iteration: 599/600] [Loss: 0.336] [Accuracy: 91.64]\n",
      "[Shal] [Epoch 8] [Iteration: 599/600] [Loss: 0.394] [Accuracy: 90.15]\n",
      "[Deep] [Epoch 9] [Iteration: 199/600] [Loss: 0.234] [Accuracy: 91.62]\n",
      "[Shal] [Epoch 9] [Iteration: 199/600] [Loss: 0.318] [Accuracy: 90.19]\n",
      "[Deep] [Epoch 9] [Iteration: 399/600] [Loss: 0.387] [Accuracy: 91.56]\n",
      "[Shal] [Epoch 9] [Iteration: 399/600] [Loss: 0.470] [Accuracy: 90.35]\n",
      "[Deep] [Epoch 9] [Iteration: 599/600] [Loss: 0.360] [Accuracy: 91.68]\n",
      "[Shal] [Epoch 9] [Iteration: 599/600] [Loss: 0.462] [Accuracy: 90.35]\n",
      "[Deep] [Epoch 10] [Iteration: 199/600] [Loss: 0.442] [Accuracy: 91.66]\n",
      "[Shal] [Epoch 10] [Iteration: 199/600] [Loss: 0.545] [Accuracy: 90.41]\n",
      "[Deep] [Epoch 10] [Iteration: 399/600] [Loss: 0.384] [Accuracy: 91.67]\n",
      "[Shal] [Epoch 10] [Iteration: 399/600] [Loss: 0.458] [Accuracy: 90.38]\n",
      "[Deep] [Epoch 10] [Iteration: 599/600] [Loss: 0.185] [Accuracy: 91.76]\n",
      "[Shal] [Epoch 10] [Iteration: 599/600] [Loss: 0.273] [Accuracy: 90.51]\n",
      "[Deep] [Epoch 11] [Iteration: 199/600] [Loss: 0.276] [Accuracy: 91.75]\n",
      "[Shal] [Epoch 11] [Iteration: 199/600] [Loss: 0.347] [Accuracy: 90.45]\n",
      "[Deep] [Epoch 11] [Iteration: 399/600] [Loss: 0.222] [Accuracy: 91.77]\n",
      "[Shal] [Epoch 11] [Iteration: 399/600] [Loss: 0.316] [Accuracy: 90.57]\n",
      "[Deep] [Epoch 11] [Iteration: 599/600] [Loss: 0.204] [Accuracy: 91.81]\n",
      "[Shal] [Epoch 11] [Iteration: 599/600] [Loss: 0.277] [Accuracy: 90.57]\n",
      "[Deep] [Epoch 12] [Iteration: 199/600] [Loss: 0.416] [Accuracy: 91.85]\n",
      "[Shal] [Epoch 12] [Iteration: 199/600] [Loss: 0.459] [Accuracy: 90.61]\n",
      "[Deep] [Epoch 12] [Iteration: 399/600] [Loss: 0.218] [Accuracy: 91.78]\n",
      "[Shal] [Epoch 12] [Iteration: 399/600] [Loss: 0.265] [Accuracy: 90.57]\n",
      "[Deep] [Epoch 12] [Iteration: 599/600] [Loss: 0.192] [Accuracy: 91.84]\n",
      "[Shal] [Epoch 12] [Iteration: 599/600] [Loss: 0.275] [Accuracy: 90.60]\n",
      "[Deep] [Epoch 13] [Iteration: 199/600] [Loss: 0.170] [Accuracy: 91.87]\n",
      "[Shal] [Epoch 13] [Iteration: 199/600] [Loss: 0.239] [Accuracy: 90.68]\n",
      "[Deep] [Epoch 13] [Iteration: 399/600] [Loss: 0.205] [Accuracy: 91.90]\n",
      "[Shal] [Epoch 13] [Iteration: 399/600] [Loss: 0.318] [Accuracy: 90.72]\n",
      "[Deep] [Epoch 13] [Iteration: 599/600] [Loss: 0.444] [Accuracy: 91.94]\n",
      "[Shal] [Epoch 13] [Iteration: 599/600] [Loss: 0.481] [Accuracy: 90.68]\n",
      "[Deep] [Epoch 14] [Iteration: 199/600] [Loss: 0.259] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 14] [Iteration: 199/600] [Loss: 0.346] [Accuracy: 90.81]\n",
      "[Deep] [Epoch 14] [Iteration: 399/600] [Loss: 0.338] [Accuracy: 91.86]\n",
      "[Shal] [Epoch 14] [Iteration: 399/600] [Loss: 0.351] [Accuracy: 90.74]\n",
      "[Deep] [Epoch 14] [Iteration: 599/600] [Loss: 0.225] [Accuracy: 91.90]\n",
      "[Shal] [Epoch 14] [Iteration: 599/600] [Loss: 0.278] [Accuracy: 90.79]\n",
      "[Deep] [Epoch 15] [Iteration: 199/600] [Loss: 0.339] [Accuracy: 92.00]\n",
      "[Shal] [Epoch 15] [Iteration: 199/600] [Loss: 0.419] [Accuracy: 90.81]\n",
      "[Deep] [Epoch 15] [Iteration: 399/600] [Loss: 0.252] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 15] [Iteration: 399/600] [Loss: 0.283] [Accuracy: 90.79]\n",
      "[Deep] [Epoch 15] [Iteration: 599/600] [Loss: 0.273] [Accuracy: 92.02]\n",
      "[Shal] [Epoch 15] [Iteration: 599/600] [Loss: 0.328] [Accuracy: 90.85]\n",
      "[Deep] [Epoch 16] [Iteration: 199/600] [Loss: 0.245] [Accuracy: 91.85]\n",
      "[Shal] [Epoch 16] [Iteration: 199/600] [Loss: 0.300] [Accuracy: 90.90]\n",
      "[Deep] [Epoch 16] [Iteration: 399/600] [Loss: 0.299] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 16] [Iteration: 399/600] [Loss: 0.368] [Accuracy: 90.94]\n",
      "[Deep] [Epoch 16] [Iteration: 599/600] [Loss: 0.110] [Accuracy: 91.95]\n",
      "[Shal] [Epoch 16] [Iteration: 599/600] [Loss: 0.209] [Accuracy: 90.99]\n",
      "[Deep] [Epoch 17] [Iteration: 199/600] [Loss: 0.201] [Accuracy: 91.95]\n",
      "[Shal] [Epoch 17] [Iteration: 199/600] [Loss: 0.287] [Accuracy: 91.05]\n",
      "[Deep] [Epoch 17] [Iteration: 399/600] [Loss: 0.381] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 17] [Iteration: 399/600] [Loss: 0.423] [Accuracy: 90.96]\n",
      "[Deep] [Epoch 17] [Iteration: 599/600] [Loss: 0.464] [Accuracy: 92.02]\n",
      "[Shal] [Epoch 17] [Iteration: 599/600] [Loss: 0.480] [Accuracy: 90.98]\n",
      "[Deep] [Epoch 18] [Iteration: 199/600] [Loss: 0.244] [Accuracy: 91.99]\n",
      "[Shal] [Epoch 18] [Iteration: 199/600] [Loss: 0.344] [Accuracy: 91.04]\n",
      "[Deep] [Epoch 18] [Iteration: 399/600] [Loss: 0.246] [Accuracy: 92.15]\n",
      "[Shal] [Epoch 18] [Iteration: 399/600] [Loss: 0.293] [Accuracy: 91.08]\n",
      "[Deep] [Epoch 18] [Iteration: 599/600] [Loss: 0.173] [Accuracy: 92.01]\n",
      "[Shal] [Epoch 18] [Iteration: 599/600] [Loss: 0.225] [Accuracy: 91.10]\n",
      "[Deep] [Epoch 19] [Iteration: 199/600] [Loss: 0.273] [Accuracy: 92.05]\n",
      "[Shal] [Epoch 19] [Iteration: 199/600] [Loss: 0.370] [Accuracy: 91.14]\n",
      "[Deep] [Epoch 19] [Iteration: 399/600] [Loss: 0.357] [Accuracy: 91.97]\n",
      "[Shal] [Epoch 19] [Iteration: 399/600] [Loss: 0.448] [Accuracy: 91.17]\n",
      "[Deep] [Epoch 19] [Iteration: 599/600] [Loss: 0.337] [Accuracy: 92.06]\n",
      "[Shal] [Epoch 19] [Iteration: 599/600] [Loss: 0.407] [Accuracy: 91.19]\n",
      "[Deep] [Epoch 20] [Iteration: 199/600] [Loss: 0.144] [Accuracy: 92.07]\n",
      "[Shal] [Epoch 20] [Iteration: 199/600] [Loss: 0.257] [Accuracy: 91.19]\n",
      "[Deep] [Epoch 20] [Iteration: 399/600] [Loss: 0.333] [Accuracy: 92.10]\n",
      "[Shal] [Epoch 20] [Iteration: 399/600] [Loss: 0.365] [Accuracy: 91.31]\n",
      "[Deep] [Epoch 20] [Iteration: 599/600] [Loss: 0.223] [Accuracy: 92.04]\n",
      "[Shal] [Epoch 20] [Iteration: 599/600] [Loss: 0.291] [Accuracy: 91.20]\n",
      "[Deep] [Epoch 21] [Iteration: 199/600] [Loss: 0.281] [Accuracy: 92.18]\n",
      "[Shal] [Epoch 21] [Iteration: 199/600] [Loss: 0.358] [Accuracy: 91.24]\n",
      "[Deep] [Epoch 21] [Iteration: 399/600] [Loss: 0.252] [Accuracy: 92.10]\n",
      "[Shal] [Epoch 21] [Iteration: 399/600] [Loss: 0.343] [Accuracy: 91.30]\n",
      "[Deep] [Epoch 21] [Iteration: 599/600] [Loss: 0.209] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 21] [Iteration: 599/600] [Loss: 0.279] [Accuracy: 91.34]\n",
      "[Deep] [Epoch 22] [Iteration: 199/600] [Loss: 0.218] [Accuracy: 92.16]\n",
      "[Shal] [Epoch 22] [Iteration: 199/600] [Loss: 0.270] [Accuracy: 91.34]\n",
      "[Deep] [Epoch 22] [Iteration: 399/600] [Loss: 0.553] [Accuracy: 92.15]\n",
      "[Shal] [Epoch 22] [Iteration: 399/600] [Loss: 0.540] [Accuracy: 91.30]\n",
      "[Deep] [Epoch 22] [Iteration: 599/600] [Loss: 0.302] [Accuracy: 92.07]\n",
      "[Shal] [Epoch 22] [Iteration: 599/600] [Loss: 0.358] [Accuracy: 91.41]\n",
      "[Deep] [Epoch 23] [Iteration: 199/600] [Loss: 0.323] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 23] [Iteration: 199/600] [Loss: 0.391] [Accuracy: 91.35]\n",
      "[Deep] [Epoch 23] [Iteration: 399/600] [Loss: 0.358] [Accuracy: 92.21]\n",
      "[Shal] [Epoch 23] [Iteration: 399/600] [Loss: 0.394] [Accuracy: 91.34]\n",
      "[Deep] [Epoch 23] [Iteration: 599/600] [Loss: 0.332] [Accuracy: 92.17]\n",
      "[Shal] [Epoch 23] [Iteration: 599/600] [Loss: 0.369] [Accuracy: 91.40]\n",
      "[Deep] [Epoch 24] [Iteration: 199/600] [Loss: 0.297] [Accuracy: 92.23]\n",
      "[Shal] [Epoch 24] [Iteration: 199/600] [Loss: 0.314] [Accuracy: 91.46]\n",
      "[Deep] [Epoch 24] [Iteration: 399/600] [Loss: 0.308] [Accuracy: 92.21]\n",
      "[Shal] [Epoch 24] [Iteration: 399/600] [Loss: 0.389] [Accuracy: 91.41]\n",
      "[Deep] [Epoch 24] [Iteration: 599/600] [Loss: 0.423] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 24] [Iteration: 599/600] [Loss: 0.444] [Accuracy: 91.43]\n",
      "[Deep] [Epoch 25] [Iteration: 199/600] [Loss: 0.206] [Accuracy: 92.24]\n",
      "[Shal] [Epoch 25] [Iteration: 199/600] [Loss: 0.257] [Accuracy: 91.52]\n",
      "[Deep] [Epoch 25] [Iteration: 399/600] [Loss: 0.155] [Accuracy: 92.22]\n",
      "[Shal] [Epoch 25] [Iteration: 399/600] [Loss: 0.214] [Accuracy: 91.44]\n",
      "[Deep] [Epoch 25] [Iteration: 599/600] [Loss: 0.200] [Accuracy: 92.30]\n",
      "[Shal] [Epoch 25] [Iteration: 599/600] [Loss: 0.237] [Accuracy: 91.43]\n",
      "[Deep] [Epoch 26] [Iteration: 199/600] [Loss: 0.317] [Accuracy: 92.29]\n",
      "[Shal] [Epoch 26] [Iteration: 199/600] [Loss: 0.388] [Accuracy: 91.46]\n",
      "[Deep] [Epoch 26] [Iteration: 399/600] [Loss: 0.283] [Accuracy: 92.31]\n",
      "[Shal] [Epoch 26] [Iteration: 399/600] [Loss: 0.355] [Accuracy: 91.46]\n",
      "[Deep] [Epoch 26] [Iteration: 599/600] [Loss: 0.196] [Accuracy: 92.28]\n",
      "[Shal] [Epoch 26] [Iteration: 599/600] [Loss: 0.205] [Accuracy: 91.48]\n",
      "[Deep] [Epoch 27] [Iteration: 199/600] [Loss: 0.181] [Accuracy: 92.19]\n",
      "[Shal] [Epoch 27] [Iteration: 199/600] [Loss: 0.257] [Accuracy: 91.50]\n",
      "[Deep] [Epoch 27] [Iteration: 399/600] [Loss: 0.281] [Accuracy: 92.20]\n",
      "[Shal] [Epoch 27] [Iteration: 399/600] [Loss: 0.407] [Accuracy: 91.56]\n",
      "[Deep] [Epoch 27] [Iteration: 599/600] [Loss: 0.350] [Accuracy: 92.29]\n",
      "[Shal] [Epoch 27] [Iteration: 599/600] [Loss: 0.363] [Accuracy: 91.53]\n",
      "[Deep] [Epoch 28] [Iteration: 199/600] [Loss: 0.208] [Accuracy: 92.38]\n",
      "[Shal] [Epoch 28] [Iteration: 199/600] [Loss: 0.258] [Accuracy: 91.57]\n",
      "[Deep] [Epoch 28] [Iteration: 399/600] [Loss: 0.210] [Accuracy: 92.28]\n",
      "[Shal] [Epoch 28] [Iteration: 399/600] [Loss: 0.258] [Accuracy: 91.53]\n",
      "[Deep] [Epoch 28] [Iteration: 599/600] [Loss: 0.229] [Accuracy: 92.15]\n",
      "[Shal] [Epoch 28] [Iteration: 599/600] [Loss: 0.300] [Accuracy: 91.58]\n",
      "[Deep] [Epoch 29] [Iteration: 199/600] [Loss: 0.201] [Accuracy: 92.27]\n",
      "[Shal] [Epoch 29] [Iteration: 199/600] [Loss: 0.238] [Accuracy: 91.61]\n",
      "[Deep] [Epoch 29] [Iteration: 399/600] [Loss: 0.280] [Accuracy: 92.37]\n",
      "[Shal] [Epoch 29] [Iteration: 399/600] [Loss: 0.380] [Accuracy: 91.57]\n",
      "[Deep] [Epoch 29] [Iteration: 599/600] [Loss: 0.316] [Accuracy: 92.29]\n",
      "[Shal] [Epoch 29] [Iteration: 599/600] [Loss: 0.376] [Accuracy: 91.58]\n"
     ]
    }
   ],
   "source": [
    "# Step 7. Train Model\n",
    "# 임의의 학습 이미지를 가져옵니다\n",
    "dataiter = iter(train_loader)\n",
    "images, _ = dataiter.next()\n",
    "writer_deep.add_graph(model_deep, images.view(-1, 28 * 28))\n",
    "writer_shallow.add_graph(model_shallow, images.view(-1, 28 * 28))\n",
    "\n",
    "loss_deep = 0\n",
    "loss_shallow = 0\n",
    "total_iter = 0\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    iter = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        labels = labels\n",
    "        images = images.to(devices_id)\n",
    "        labels = labels.to(devices_id)\n",
    "\n",
    "        optimizer_deep.zero_grad()\n",
    "        optimizer_shallow.zero_grad()\n",
    "        outputs_deep = model_deep(images)\n",
    "        outputs_shallow = model_shallow(images)\n",
    "        # Calc loss\n",
    "        loss_deep = criterion(outputs_deep, labels)\n",
    "        loss_shallow = criterion(outputs_shallow, labels)\n",
    "        # Back-propagation\n",
    "        loss_deep.backward()\n",
    "        loss_shallow.backward()\n",
    "        # Updating wegihts\n",
    "        optimizer_deep.step()\n",
    "        optimizer_shallow.step()\n",
    "\n",
    "        total_iter += 1\n",
    "        if total_iter < int(600*epochs - 10):\n",
    "            writer_deep.add_scalar('Train/Loss', loss_deep, total_iter)\n",
    "            writer_shallow.add_scalar('Train/Loss', loss_shallow, total_iter)\n",
    "\n",
    "        iter += 1\n",
    "        if iter % 200 == 0:\n",
    "            # calculate Accuracy\n",
    "            correct_deep = 0\n",
    "            correct_shallow = 0\n",
    "            total = 0\n",
    "\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28 * 28)\n",
    "                images = images.to(devices_id)\n",
    "                \n",
    "                outputs_deep = model_deep(images)\n",
    "                outputs_shallow = model_shallow(images)\n",
    "\n",
    "                _, predicted_deep = torch.max(outputs_deep.data, 1)\n",
    "                _, predicted_shallow = torch.max(outputs_shallow.data, 1)\n",
    "                total += labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                predicted_deep = predicted_deep.cpu()\n",
    "                predicted_shallow = predicted_shallow.cpu()\n",
    "\n",
    "                correct_deep += (predicted_deep == labels).sum()\n",
    "                correct_shallow += (predicted_shallow == labels).sum()\n",
    "\n",
    "            accuracy_deep = 100 * correct_deep / total\n",
    "            accuracy_shallow = 100 * correct_shallow / total\n",
    "\n",
    "            print(\n",
    "                f\"[Deep] [Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss_deep.item():.3f}] [Accuracy: {accuracy_deep:.2f}]\"\n",
    "            )\n",
    "            print(\n",
    "                f\"[Shal] [Epoch {epoch}] [Iteration: {i}/{len(train_loader)}] [Loss: {loss_shallow.item():.3f}] [Accuracy: {accuracy_shallow:.2f}]\"\n",
    "            )\n",
    "\n",
    "writer_deep.close()\n",
    "writer_shallow.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}